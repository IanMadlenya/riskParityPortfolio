---
title: "Efficient Computation of Risk and Jacobians for Risk-Parity Portfolio"
author: 
- name: "Daniel P. Palomar and Vinicius Ze"
  affiliation: "Hong Kong University of Science and Technology (HKUST)"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    base_format: prettydoc::html_pretty
    theme: tactile
    highlight: vignette
    fig_caption: yes
    number_sections: no
    toc: yes
    toc_depth: 2
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
header-includes:
  \allowdisplaybreaks
indent: yes
csl: ieee.csl
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{Efficient Computation of Risk and Jacobians for Risk-Parity Portfolio}
  %\VignetteKeyword{portfolio, risk-parity, risk, optimization}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.retina = 2,
  out.width = "75%",
  dpi = 96
)
knit_hooks$set(pngquant = hook_pngquant)
#Help on bookdown: https://bookdown.org/yihui/bookdown/
#rmarkdown::render("vignettes/EfficientComputations-vignette.Rmd", "bookdown::html_document2")
```

-----------
> This note contains compact expressions for the risk and corresponding
Jacobians for efficient implementation


# General risk-parity portfolio formulation
The risk-parity portfolio formulation is of the form [@FengPal2016monograph]:
$$\begin{array}{ll}
\underset{\mathbf{w}}{\textsf{minimize}} & R(\mathbf{w})\\
\textsf{subject to} & \mathbf{1}^T\mathbf{w}=1\\
 & \mathbf{w}\ge\mathbf{0},
\end{array}$$
where the risk term is of the form (double summation) $R(\mathbf{w}) = \sum_{i,j=1}^{N}(g_{ij}(\mathbf{w}))^{2}$ or simply (single summation) $R(\mathbf{w}) = \sum_{i=1}^{N}(g_{i}(\mathbf{w}))^{2}$.

This problem can be solved directly with some nonlinear solver (for which we need to be able to compute the risk term $R(\mathbf{w})$ (even better if the gradient is computed) as well as with the Successive Convex Approximation (SCA) method developed in [@FengPal2015riskparity]. The algorithm iteratively solves a sequence of QP problems of the form:
$$
\begin{array}{ll}
\underset{\mathbf{w}}{\textsf{minimize}} & \tilde{U}\left(\mathbf{w},\mathbf{w}^{k}\right)=\frac{1}{2}\mathbf{w}^T\mathbf{Q}^{k}\mathbf{w}+\mathbf{w}^T\mathbf{q}^{k}+\lambda F\left(\mathbf{w}\right)\\
\textsf{subject to} & \mathbf{1}^T\mathbf{w}=1\\
 & \mathbf{w}\ge\mathbf{0},
\end{array}
$$
where
$$\begin{aligned}
\mathbf{g}\left(\mathbf{w}^{k}\right) & \triangleq \left[g_{1}\left(\mathbf{w}^{k}\right),\dots,g_{N}\left(\mathbf{w}^{k}\right)\right]^T\\
\mathbf{A}^{k}\left(\mathbf{w}^{k}\right) &\triangleq \left[\nabla g_{1}\left(\mathbf{w}^{k}\right),\dots,\nabla g_{N}\left(\mathbf{w}^{k}\right)\right]^T,\\
\mathbf{Q}^{k} &\triangleq 2\left(\mathbf{A}^{k}\right)^T\mathbf{A}^{k}+\tau\mathbf{I},\\
\mathbf{q}^{k} &\triangleq 2\left(\mathbf{A}^{k}\right)^T\mathbf{g}\left(\mathbf{w}^{k}\right)-\mathbf{Q}^{k}\mathbf{w}^{k}.
\end{aligned}$$
To effectively implement the SCA method we need efficient computation of the risk contribution terms contained in $\mathbf{g}(\mathbf{w})$ and their gradients contained in the Jacobian matrix $\mathbf{A}(\mathbf{w}) = \left[\nabla g_{1}(\mathbf{w}),\dots,\nabla g_{N}(\mathbf{w})\right]^T$.


**Notation:**

- Define the $i$-th risk contribution: $r_i = w_i(\boldsymbol{\Sigma}\mathbf{w})_i$ (in R: ` r <- w*(Sigma %*% w)`)
- Gradient of $R(\mathbf{w})$: $\nabla_{\mathbf{w}} R = \left[ \frac{\partial R}{\partial w_1}, \ldots, \frac{\partial R}{\partial w_N} \right]^T$
- Jacobian of $\mathbf{w}$: $\textsf{J}_\mathbf{w}\mathbf{x} = \frac{\partial \mathbf{x}}{\partial \mathbf{w}^T}$ (note that the Jacobian of a scalar function is the traspose of the gradient)
- For the single index case: $\mathbf{g}(\mathbf{w}) = \left[g_1(\mathbf{w}), \ldots, g_N(\mathbf{w}) \right]^T$
- For the double index case: $\mathbf{G}(\mathbf{w}) = (g_{ij}(\mathbf{w}))$ and $\mathbf{g}(\mathbf{w})=\textsf{vec}(\mathbf{G}(\mathbf{w}))$
- M-notation [@FengPal2015riskparity]: for unification purposes, one can express the risk contributions in terms of the matrix $\mathbf{M}_i$, whose $i$-th row is equal to that of $\boldsymbol{\Sigma}$ and is zero elsewhere. Then it follows that $r_i = w_i(\boldsymbol{\Sigma}\mathbf{w})_i=\mathbf{w}^T\mathbf{M}_i\mathbf{w}$ (although for computational purposes it is far more efficient to use former expression of $r_i$ than the latter).


# Formulation "rc double-index"
Let's focus on the risk expression:
$$R(\mathbf{w}) = \sum_{i,j=1}^{N}\left(w_{i}\left(\boldsymbol{\Sigma}\mathbf{w}\right)_{i}-w_{j}\left(\boldsymbol{\Sigma}\mathbf{w}\right)_{j}\right)^{2} = \sum_{i,j=1}^{N}(r_i - r_j)^2 = 2N\sum_i r_i^2 - 2\left(\sum_i r_i\right)^2$$
which can be efficiently coded as
```{r, eval=FALSE}
risk <- 2*N*sum(r^2) - 2*sum(r)^2
```

Let's compute now the gradient of $R(\mathbf{w})$:

- $\frac{\partial R}{\partial r_i}=4(N r_i-\sum_i r_i)$ $\Longrightarrow$ $\nabla_{\mathbf{r}}R = 4(N\mathbf{r}-(\mathbf{1}^T\mathbf{r})\mathbf{1})$
- $\frac{\partial r_i}{\partial w_j}=w_i\boldsymbol{\Sigma}_{ij}+\delta_{ij}(\boldsymbol{\Sigma}\mathbf{w})_i$ $\Longrightarrow$ $\textsf{J}_{\mathbf{w}}\mathbf{r} = \textsf{Diag}(\mathbf{w})\boldsymbol{\Sigma} + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w})$
- chain rule: using Jacobians is $\textsf{J}_{\mathbf{w}} R = \textsf{J}_{\mathbf{r}}R \cdot \textsf{J}_{\mathbf{w}}\mathbf{r}$, using gradients is $(\nabla_{\mathbf{w}} R)^T = (\nabla_{\mathbf{r}}R)^T \cdot \textsf{J}_{\mathbf{w}}\mathbf{r}$ or, more conveniently, $\nabla_{\mathbf{w}}R = (\textsf{J}_{\mathbf{w}}\mathbf{r})^T \cdot \nabla_{\mathbf{r}}R$:
$$\nabla_{\mathbf{w}}R = 4(\boldsymbol{\Sigma}\textsf{Diag}(\mathbf{w}) + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w})) (N\mathbf{r}-(\mathbf{1}^T\mathbf{r})\mathbf{1}),$$
which can be coded as `risk_grad <- 4*(t(Sigma*w) + diag(Sigma %*% w)) %*% (N*r-sum(r))`. A better way to code it is:

```{r, eval=FALSE}
Sigma_w <- Sigma %*% w  # this has already been computed for the computation of r
v <- N*r-sum(r)
risk_grad <- as.vector(4*(Sigma %*% (w*v) + Sigma_w*v))
```

**For SCA:**  
However, if we are interested in implenting the SCA method, this is not enough. Then we need an expression for the risk contributions contained in $\mathbf{g}$ as well as its Jacobian matrix $\mathbf{A} = \left[\nabla g_{11},\dots,\nabla g_{NN}\right]^T$.

The risk deviations are
$$g_{ij}(\mathbf{w})=w_i(\boldsymbol{\Sigma}\mathbf{w})_{i}-w_j(\boldsymbol{\Sigma}\mathbf{w})_j = r_i - r_j,$$
which can be efficiently coded as `g <- rep(r, times = N) - rep(r, each = N)`. So another way to compute $R(\mathbf{w})$ is with `sum(g^2)`, but it's not as efficient as the previous computation since $\mathbf{g}$ has $N^2$ elements.

Matrix $\mathbf{A}$ is more involved to compute. Using the M-notation [@FengPal2015riskparity]: 
$$\nabla g_{ij}(\mathbf{w}) = (\mathbf{M}_i + \mathbf{M}_i^T - \mathbf{M}_j - \mathbf{M}_j^T)\mathbf{w}.$$ 
We can compute efficiently the vectors $\mathbf{u}_i = (\mathbf{M}_i + \mathbf{M}_i^T)\mathbf{w}$ for all $i=1,\ldots,N$ at once:
$$\begin{aligned}
\left[\mathbf{M}_1\mathbf{w}, \ldots, \mathbf{M}_N\mathbf{w}\right] & = \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w})\\
\left[\mathbf{M}_1^T\mathbf{w}, \ldots, \mathbf{M}^T_N\mathbf{w}\right] & = \boldsymbol{\Sigma}\textsf{Diag}(\mathbf{w}),
\end{aligned}$$
so that $\nabla g_{ij} = \mathbf{u}_i - \mathbf{u}_j$, where $\mathbf{u}_i$ is the $i$-th column of the $N\times N$ matrix
$$\mathbf{U} = \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w}) + \boldsymbol{\Sigma}\textsf{Diag}(\mathbf{w}).$$
Finally, to compute efficiently the Jacobian $\mathbf{A}$ without using loops we can write it as
$$\mathbf{A} = \mathbf{1}_N \otimes \mathbf{U}^T - \mathbf{U}^T \otimes \mathbf{1}_N$$
which can be compactly coded as
```{r, eval=FALSE}
Sigma_w <- Sigma %*% w  # this has already been computed for the computation of r
Ut <- diag(Sigma_w) + Sigma*w
A <- matrix(rep(t(Ut), N), ncol = N, byrow = TRUE) - matrix(rep(Ut, each = N), ncol = N)
```




# Formulation "rc-over-var vs b"
Consider now the following risk expression with a single index:
$$R(\mathbf{w}) = \sum_{i=1}^{N}\left(\frac{w_{i}\left(\boldsymbol{\Sigma}\mathbf{w}\right)_i}{\mathbf{w}^T\boldsymbol{\Sigma}\mathbf{w}}-b_i\right)^{2} = \sum_{i=1}^{N}\left(\frac{r_i}{\mathbf{1}^T\mathbf{r}}-b_i\right)^{2}$$
where $b_{i}=\frac{1}{N}$. This can be efficiently coded as
```{r, eval=FALSE}
risk <- sum((r/sum(r) - b)^2)
```

Let's compute now the gradient of $R(\mathbf{w})$:

- first, w.r.t. $\mathbf{r}$:

$$\begin{aligned}
\frac{\partial R}{\partial r_j}
 & = 2\sum_i\left(\frac{r_i}{\mathbf{1}^T\mathbf{r}}-b_i\right)\left(\frac{\delta_{ij}}{\mathbf{1}^T\mathbf{r}}-\frac{r_i}{(\mathbf{1}^T\mathbf{r})^2} \right)\\
 & = 2\sum_i\left(\frac{r_i}{\mathbf{1}^T\mathbf{r}}-b_i\right)\frac{\delta_{ij}}{\mathbf{1}^T\mathbf{r}}
 - 2\sum_i\left(\frac{r_i}{\mathbf{1}^T\mathbf{r}}-b_i\right)\frac{r_i}{(\mathbf{1}^T\mathbf{r})^2}\\
 & = \frac{2}{\mathbf{1}^T\mathbf{r}}\left(\left(\frac{r_j}{\mathbf{1}^T\mathbf{r}}-b_j\right)
 - \left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)^T\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}\right)
\end{aligned}$$
so that
$$\nabla_{\mathbf{r}}R = \frac{2}{\mathbf{1}^T\mathbf{r}}\left(\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)
 - \mathbf{1}\cdot\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)^T\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}\right)$$

- $\nabla_{\mathbf{w}}R = (\textsf{J}_{\mathbf{w}}\mathbf{r})^T \cdot \nabla_{\mathbf{r}}R$ with $\textsf{J}_{\mathbf{w}}\mathbf{r} = \textsf{Diag}(\mathbf{w})\boldsymbol{\Sigma} + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w})$:

$$\nabla_{\mathbf{w}}R = \frac{2}{\mathbf{1}^T\mathbf{x}}(\boldsymbol{\Sigma}\textsf{Diag}(\mathbf{w}) + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w}))
\left(\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)
 - \mathbf{1}\cdot\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)^T\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}\right),$$
which can be coded as  
```{r, eval=FALSE}
Sigma_w <- Sigma %*% w  # this has already been computed for the computation of r
sum_r <- sum(r)
r_b <- r/sum_r - b
v <- r_b - as.numeric(r_b %*% r)/sum_r
risk_grad <- (2/sum_r) * as.vector(Sigma %*% (w*v) + Sigma_w*v)
```


**For SCA:**  
The risk deviations are
$$g_i(\mathbf{w}) = \frac{r_i}{\mathbf{1}^T\mathbf{r}}-b_i,$$
which can be efficiently coded as `g <- r/sum(r) - b`.

Matrix $\mathbf{A}$ follows from the M-notation [@FengPal2015riskparity]: 
$$\nabla g_{i}(\mathbf{w}) = \frac{(\mathbf{w}^T\boldsymbol{\Sigma}\mathbf{w})(\mathbf{M}_i + \mathbf{M}_i^T)\mathbf{w} - (\mathbf{w}^T\mathbf{M}_i\mathbf{w})(2\boldsymbol{\Sigma})\mathbf{w}}{(\mathbf{w}^T\boldsymbol{\Sigma}\mathbf{w})^2}.$$ 
Recall that $\mathbf{u}_i = (\mathbf{M}_i + \mathbf{M}_i^T)\mathbf{w}$ can be conveniently expressed as the $i$-th column of the $N\times N$ matrix
$$\mathbf{U} = \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w}) + \boldsymbol{\Sigma}\textsf{Diag}(\mathbf{w}).$$
We can then write
$$\nabla g_{i}(\mathbf{w}) = \frac{(\mathbf{1}^T\mathbf{r})\mathbf{u}_i - 2r_i\boldsymbol{\Sigma}\mathbf{w}}{(\mathbf{1}^T\mathbf{r})^2} = \frac{\mathbf{u}_i}{\mathbf{1}^T\mathbf{r}} - 2\frac{r_i}{(\mathbf{1}^T\mathbf{r})^2}\boldsymbol{\Sigma}\mathbf{w}$$
and
$$\mathbf{A} = \frac{\mathbf{U}^T}{\mathbf{1}^T\mathbf{r}} - \frac{2}{(\mathbf{1}^T\mathbf{r})^2}\mathbf{r}\mathbf{w}^T\boldsymbol{\Sigma}$$
which can be compactly coded as
```{r, eval=FALSE}
sum_r <- sum(r)
Sigma_w <- Sigma %*% w  # this has already been computed for the computation of r
Ut <- diag(Sigma_w) + Sigma*w
A <- Ut/sum_r - 2/(sum_r^2) * r %o% Sigma_w
```




**[This is wrong!!!] My old derivation**:
Matrix $\mathbf{A}=\left[\nabla g_1,\dots,\nabla g_N\right]^T$ is more involved to compute (we will avoid the M-notation as it gets too involved). The partial derivative is
$$\frac{\partial g_i}{\partial x_j} = \frac{\delta_{ij}}{\mathbf{1}^T\mathbf{x}} - \frac{x_i}{(\mathbf{1}^T\mathbf{x})^2}$$
and the gradient is 
$$\nabla_\mathbf{x} g_i = \frac{1}{\mathbf{1}^T\mathbf{x}}\left(\mathbf{e}_i-\frac{\mathbf{x}}{\mathbf{1}^T\mathbf{x}}\right)$$
where $\mathbf{e}_i$ is the $i$-canonical vector (all-zero except a 1 at the $i$-th position). We can then write the Jacobian as
$$\textsf{J}_{\mathbf{x}}\mathbf{g} = \frac{1}{\mathbf{1}^T\mathbf{x}}\left(\mathbf{I}-\frac{\mathbf{1}\otimes \mathbf{x}^T}{\mathbf{1}^T\mathbf{x}}\right),$$ where $\otimes$ denotes Kronecker product.
Finally we can use the Jacobian chain rule (recall that $\textsf{J}_{\mathbf{w}}\mathbf{x} = \textsf{Diag}(\mathbf{w})\boldsymbol{\Sigma} + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w})$):
$$\mathbf{A} \triangleq \textsf{J}_{\mathbf{w}}\mathbf{g} = \frac{1}{\mathbf{1}^T\mathbf{x}}\left(\mathbf{I}-\frac{\mathbf{1}\otimes \mathbf{x}^T}{\mathbf{1}^T\mathbf{x}}\right) \cdot \left(\textsf{Diag}(\mathbf{w})\boldsymbol{\Sigma} + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w})\right),$$
which can be more efficiently expressed as
$$\mathbf{A} = \frac{1}{\mathbf{1}^T\mathbf{x}}\left(\left(\textsf{Diag}(\mathbf{w})\boldsymbol{\Sigma} + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w})\right)-\frac{1}{\mathbf{1}^T\mathbf{x}}\mathbf{1}\otimes\left(\mathbf{x}^T\left(\textsf{Diag}(\mathbf{w})\boldsymbol{\Sigma} + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w})\right)\right)\right).$$
This can be efficiently coded as
```{r, eval=FALSE}
sum_r <- sum(r)
U <- t(Sigma*w) + diag(Sigma_w)
A <- (1/sum_r) * (U - (1/sum_r)*matrix((t(x) %*% U), N, N, byrow = TRUE)
```


# Formulation "rc-over-sd vs b-times-sd"
Consider now the following risk expression:
$$R(\mathbf{w}) = \sum_{i=1}^{N}\left(\frac{w_{i}\left(\boldsymbol{\Sigma}\mathbf{w}\right)_i}{\sqrt{\mathbf{w}^T\boldsymbol{\Sigma}\mathbf{w}}}-b_i\sqrt{\mathbf{w}^T\boldsymbol{\Sigma}\mathbf{w}}\right)^{2} = \sum_{i=1}^{N}\left(\frac{r_i}{\sqrt{\mathbf{1}^T\mathbf{r}}}-b_i\sqrt{\mathbf{1}^T\mathbf{r}}\right)^{2}$$
where $b_{i}=\frac{1}{N}$. This can be efficiently coded as
```{r, eval=FALSE}
sqrt_sum_r <- sqrt(sum(r))
risk <- sum((r/sqrt_sum_r - b*sqrt_sum_r)^2)
```

Let's compute now the gradient of $R(\mathbf{w})$:

- we will use $\frac{\partial\sqrt{\mathbf{1}^T\mathbf{r}}}{\partial r_j} = \frac{1}{2\sqrt{\mathbf{1}^T\mathbf{r}}}$ and $\frac{\partial\left(1/\sqrt{\mathbf{1}^T\mathbf{r}}\right)}{\partial r_j} = \frac{-1}{2\left(\mathbf{1}^T\mathbf{r}\right)\sqrt{\mathbf{1}^T\mathbf{r}}}$

- first, w.r.t. $\mathbf{r}$:

$$\begin{aligned}
\frac{\partial R}{\partial r_j}
 & = 2\sum_i\left(\frac{r_i}{\sqrt{\mathbf{1}^T\mathbf{r}}}-b_i\sqrt{\mathbf{1}^T\mathbf{r}}\right)\left(\frac{\delta_{ij}}{\sqrt{\mathbf{1}^T\mathbf{r}}} - \frac{r_i}{\mathbf{1}^T\mathbf{r}}\frac{1}{2\sqrt{\mathbf{1}^T\mathbf{r}}} - b_i\frac{1}{2\sqrt{\mathbf{1}^T\mathbf{r}}}\right)\\
 & = 2\sum_i\left(\frac{r_i}{\sqrt{\mathbf{1}^T\mathbf{r}}}-b_i\sqrt{\mathbf{1}^T\mathbf{r}}\right)\frac{\delta_{ij}}{\sqrt{\mathbf{1}^T\mathbf{r}}} - 2\sum_i\left(\frac{r_i}{\sqrt{\mathbf{1}^T\mathbf{r}}}-b_i\sqrt{\mathbf{1}^T\mathbf{r}}\right)\left(\frac{r_i}{\mathbf{1}^T\mathbf{r}}-b_i\right)\frac{1}{2\sqrt{\mathbf{1}^T\mathbf{r}}}\\
 & = 2\sum_i\left(\frac{r_i}{\mathbf{1}^T\mathbf{r}}-b_i\right)\delta_{ij} - \sum_i\left(\frac{r_i}{\mathbf{1}^T\mathbf{r}}-b_i\right)\left(\frac{r_i}{\mathbf{1}^T\mathbf{r}}-b_i\right)\\
 & = 2\left(\frac{r_j}{\mathbf{1}^T\mathbf{r}}-b_j\right) - \left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)^T\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)
\end{aligned}$$
so that
$$\nabla_{\mathbf{r}}R = 2\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)
 - \mathbf{1}\cdot\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)^T\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)$$

- $\nabla_{\mathbf{w}}R = (\textsf{J}_{\mathbf{w}}\mathbf{r})^T \cdot \nabla_{\mathbf{r}}R$ with $\textsf{J}_{\mathbf{w}}\mathbf{r} = \textsf{Diag}(\mathbf{w})\boldsymbol{\Sigma} + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w})$:

$$\nabla_{\mathbf{w}}R = (\boldsymbol{\Sigma}\textsf{Diag}(\mathbf{w}) + \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w}))
\left(2\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)
 - \mathbf{1}\cdot\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)^T\left(\frac{\mathbf{r}}{\mathbf{1}^T\mathbf{r}}-\mathbf{b}\right)\right),$$
which can be coded as
```{r, eval=FALSE}
Sigma_w <- Sigma %*% w  # this has already been computed for the computation of r
sum_r <- sum(r)
r_b <- r/sum_r - b
v <- r_b - as.numeric(r_b %*% r)/sum_r
risk_grad <- as.vector(Sigma %*% (w*v) + Sigma_w*v)
```

**For SCA:**  
The risk deviation of the $i$-th asset is
$$
\begin{equation}
g_i(\mathbf{w}) = \frac{r_i}{\sqrt{\mathbf{1}^{T}\mathbf{r}}} - b_i \sqrt{\mathbf{1}^{T}\mathbf{r}}
\end{equation}
$$
which can be efficiently coded as `g <- r/sum(r) - b`.
```{r, eval=FALSE}
sqrt_sum_r <- sqrt(sum(r))
g <- r/sqrt_sum_r - b*sqrt_sum_r
```

Matrix $\mathbf{A}$ follows from the M-notation [@FengPal2015riskparity]: 
$$\nabla g_{i}(\mathbf{w}) = \frac{(\mathbf{w}^T\boldsymbol{\Sigma}\mathbf{w})(\mathbf{M}_i + \mathbf{M}_i^T)\mathbf{w} - (\mathbf{w}^T\mathbf{M}_i\mathbf{w})\boldsymbol{\Sigma}\mathbf{w}}{(\mathbf{w}^T\boldsymbol{\Sigma}\mathbf{w})^{3/2}} - b_i\frac{\boldsymbol{\Sigma}\mathbf{w}}{\sqrt{\mathbf{w}^T\boldsymbol{\Sigma}\mathbf{w}}}.$$ 


Recall that $\mathbf{u}_i = (\mathbf{M}_i + \mathbf{M}_i^T)\mathbf{w}$ can be conveniently expressed as the $i$-th column of the $N\times N$ matrix
$$\mathbf{U} = \textsf{Diag}(\boldsymbol{\Sigma}\mathbf{w}) + \boldsymbol{\Sigma}\textsf{Diag}(\mathbf{w}).$$
We can then write
$$\nabla g_{i}(\mathbf{w}) = \frac{(\mathbf{1}^T\mathbf{r})\mathbf{u}_i - r_i\boldsymbol{\Sigma}\mathbf{w}}{(\mathbf{1}^T\mathbf{r})^{3/2}} - b_i\frac{\boldsymbol{\Sigma}\mathbf{w}}{\sqrt{\mathbf{1}^T\mathbf{r}}} = \frac{\mathbf{u}_i}{\sqrt{\mathbf{1}^T\mathbf{r}}} - \left(\frac{r_i}{(\mathbf{1}^T\mathbf{r})^{3/2}}+\frac{b_i}{\sqrt{\mathbf{1}^T\mathbf{r}}}\right)\boldsymbol{\Sigma}\mathbf{w}$$
and
$$\mathbf{A} = \frac{\mathbf{U}^T}{\sqrt{\mathbf{1}^T\mathbf{r}}} - \left(\frac{\mathbf{r}}{(\mathbf{1}^T\mathbf{r})^{3/2}}+\frac{\mathbf{b}}{\sqrt{\mathbf{1}^T\mathbf{r}}}\right)\mathbf{w}^T\boldsymbol{\Sigma}$$
which can be compactly coded as
```{r, eval=FALSE}
sum_r <- sum(r)
Sigma_w <- Sigma %*% w  # this has already been computed for the computation of r
Ut <- diag(Sigma_w) + Sigma*w
A <- Ut/sqrt(sum_r) - (r/(sum_r^(3/2)) + b/sqrt(sum_r)) %o% Sigma_w
```





# References {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent



